## 날짜: 2025-03-24

### 스크럼
- 금일 일정 공유
- 모델 진행상황 및 Colab 환경에서의 DB 연동

### 새로 배운 내용
#### 주제 1: AI 전체적인 서비스 개요
- Alex께 멘토링 받은 내용
- AI 팀프로젝트를 할 때, 메모리나 자원과 같이 필요한 부분을 다른 파트에게 어떻게 얘기하고 의논해야할 지 질문을 했다.
- 우선 궁금했던 자원의 필요성에 대해 적으면<br>
    1) 온 디바이스: 디바이스의 메모리에 맡기는 방식인데, 아마 앱 개발을 하시는 분이 적을듯 하여 안할 것 같다
    2) GPU: Huggingface의 모델이나 기타 모델 등에서 요구하는 그래픽카드, 혹은 VRAM 사양이 들어있어서 이에 맞추는 편이라고 한다
    3) CPU: 모델을 잘 다듬고, 값을 제공하는 부분(학습은 필요 없음)을 API로 처리하여 제공하는 형태. 그런데 이걸 CPU에게 연산을 맡기는 쪽을 추천한다고 하셨다.
  - 관련해서 왜 자원을 사용하는가는 -> parameter를 ram에 다 올려두고 이를 한번에 계산을 하는 방식이기 때문에 자원이 많이 필요하다고 한다.
  - 그런데 이렇게하면 너무 큰 모델의 경우 1대에 이를 다 올릴 수 없기 때문에, MoE라는 방식으로 작은 서버에 '너는 이거 전문가'라는 느낌으로 재분배하여 처리하는 일을 요즘 큰 모델들이 한다고 한다(우리가 하는 정도는 절대 아님)<br>

- 그리고 서비스 배포와 관련하여 얘기한 내용은 백엔드(Spring)과 AI 백엔드(FastAPI)의 통신을 어떻게 가져가는가였는데, 2가지 정도로 볼 수 있는 것 같다
  - 1) FE에 AI를 직접 붙이기
    - 이렇게 하면 FE 쪽에서 좀 헷갈리긴 하는데, BE의 중간 전달 역할이 줄어들긴 한다고 한다.
    - 대신 AI 쪽에서 BE의 역할을 좀 많이 지원해야하는 문제가 생기는 듯 하다(대표적으로 통신 속도)
    - 개인적으로는 이런 방식이 친화적인 응답을 빠르게 제공하는 서비스가 아니면 굳이 이렇게 하는게 필요한가라는 생각이 들었다 (필요하면 하겠지만)
  - 2) FE - BE - AI 방식
    - BE와 AI 사이에서 Server to Server 방식으로 가져가는 방식이다
    - 해커톤 때 사용해보려는 방식이였는데, 이 상황에서 Streaming까지 가져가려면 '비동기'를 가져가는게 좋아보였다
    - 우선 비동기의 경우, FE > BE > AI로 오면 AI가 대기를 부탁하고 이를 AI > BE > FE로 전달되는 방식<br>
        이후 요청이 완료되면 이를 push로 보내서 연락하는 방식이라고 한다<br>
        여기서 그냥 stream을 열어두어도 되는데 이건 서버가 힘들어하는 방식이고, 좋은 방식은 session처럼 어디서 요청이 왔는지 기록해두었다가 이를 BE에게 넘겨서 여기로 보내줘 라고 하는 방식이 좋아보였다
    - "통신 방식"도 문제가 있는거 같은데, HTTP만 생각을 해봤는데 BE와 AI는 서버 대 서버이니 **Redis Public  Subscribe**나 **Kapka** 등의 기능을 활용하면 더 좋을 듯 하다 (push 알림 이쁘게)
    - 그리고 위 방식이 좋은게, AI가 기계적인 답변을 보내거나, 특히 "사용자의 입력이 길게 오는" 이런 경우에 이를 다듬어줄 **오케스트레이션** 기능을 BE에서 담당해주면 좋다고 한다.

#### 주제 2: Loss의 의미
- Loss가 편미분이 진행될 때는 각 입력과 관련된 가중치에게 어떤 식으로 적용이 되는지 어느 정도 개념을 잡은 것 같았다
- 그런데 과제를 할 때도 의문이였던게, 이 Loss "값" 이라는게 결국 우리에게 어떤 의미를 주는 것인지가 궁금했다
- 결론적으로 이 "값의 의미"를 이해하는 것 자체는 매우 어려운 일이라고 하셨다
- Loss라는 것 자체가 function이지만 이 값의 결론은 Scala로 나오는 만큼,<br>
  입력이 스칼라라면 어느 정도 선형의 의미가 보여서 괜찮을지 몰라도, 행렬이 입력이 되는 경우는 그 값의 의미 해석이 매우 복잡해진다고 한다
- 우선은 트랜스포머 까지의 개념을 쭉 진행하면서 추후 다시 아이디어가 떠오를 때, 이 의미라는 접근을 다시 해보는 게 좋을 거 같았다.


### 오늘의 도전 과제와 해결 방법
- 도전 과제 1: Self-Attention 까지 복습하기
  - 우선 RNN에서 seq2seq의 내용까지 어제 아이디어의 발전을 공부했고, 오늘은 Self-Attention 개념까지 진행했다
  - Loss의 편미분을 통해 Weight를 어떻게 Optimizer를 통해서 업데이트 한다는 건지 하나씩 확인해보았다
  - 특히 Attention에서 Self-Attention으로 넘어갈 때, 기존의 RNN에서 사용하던 h 끼리의 시점 관계를 끊는다는걸 이제라도 제대로 이해해서 다행이라고 생각한다 (개인적으로 트랜스포머 수업을 짧게 듣고 이해하는 것은 매우 어려울 것이라 생각한다... 공부하는데 어려웠다;;)


### 오늘의 회고
- 오늘까지해서 트랜스포머 이전의 Self-Attention까지 개념의 발전 및 Loss의 편미분을 기준으로 아이디어의 발전을 수식을 통해 따라와봤다
- 이제 Multi Head로 넘어가야하는 단계인데, 금일 좋은 멘토링을 받은 내용들을 복기하기 위해서 내일은 fastapi에서 비동기 통신을 공부해보려 한다
- 만약 비동기 통신과 관련된 연결을 금방 하는 경우는 조금 더 확장해서, 모델을 서버에 올려서 push 기능을 체험해보는 방향으로 진행하면 더 좋을 것 같다

### 오늘의 코테
- X (수학만 하루 종일 한 거 같다)

### 참고 자료 및 링크
- X