## 날짜: 2025-02-08

### 스크럼
- 학습 목표 1 : 수학 복습하기

### 새로 배운 내용
#### 주제 1: 군, 링, 체
- Group : 마그마 라는 기본단위(?)에서 시작해서 겹합법칙, 항등원, 역원에 대해 성립되는 것들
- Ring : 곱셈에 대해 결합법칙이 추가로 성립 + 2개의 연산
- 체 : 위에 곱셈에 대해 역원이 추가로 성립

#### 주제 2: 벡터공간, 거리공간
- 벡터공간 (vector space)
  - (V, F, +, *) = Module 이라 한다
  - *은 F x V -> V 로 정의됨
- 거리공간 (metric space)
  - 벡터공간에 norm(거리, d)를 얹어서 (V, d)의 공간을 이룬 것
  - 이 위에 위상공간(topological space)이 있음

#### 주제 3: 수학의 차원, NumPy의 차원
- 수학의 차원 : M = [[1, 2], [3, 4]] 라는 numpy 배열을 flatten하면 m = (1, 2, 3, 4)와 같이 표현 가능
  - 이 m의 수학적 차원은 dim m = 4 -> 수학적인 차원이 유지가 됨
  - 이걸 **numpy size**라고 함
- NumPy의 차원 : Order, Degree or Rank of Tensor in Math

#### 주제 4: 벡터 임베딩 (Vector Embedding)
1. 용어 정리
     - 스칼라 : 단일 숫자를 포함하는 0차원 텐서
     - 벡터 : 동일한 유형의 데이터를 나타내는 여러 스칼라를 포함하는 1차원 텐서
     - 튜플 : 두 가지 이상의 데이터 유형으로 구성된 1차 텐서
     - 행렬 : 동일한 유형의 데이터를 가진 여러 벡터를 포함하는 2차원 텐서
     - N차원 텐서(다차원 배열) : 색상 이미지 혹은 그 이상의 차원 정보를 포함하는 것들
2. 차원에 대해
     - **차원**이 우리가 직관적으로 보는 물리적인 차원과 의미가 다름
     - **벡터 공간에서 각 차원**은 물리적 공간에서 물체의 특징을 설명하는 것과 같이 **데이터의 개별 특징을 포함**
     - 그러나 모든 데이터 차원에 유용한 정보가 포함된 것은 아니기 때문에 이러한 '노이즈'를 처리하는 경우가 많음
     - 또한 이런 노이즈를 줄이거나 중복되는 정보를 생략하기 위해 '차원 축소'를 거치는 경우도 많다고 함
       - 차원 축소를 통해 모델의 속도와 효율성을 높일 수 있지만, 정확도나 정밀도에 절충점을 갖게 되는 특징이 생김
3. 차원 축소 & 노이즈 처리
     - 차원 축소 : 특징 선택, 특징 추출이 있다고 함
       - 특징 선택 : 중요도에 순위를 정해 결정하는 방법<br>
           동일한 문제인데 데이터셋에 따라 rank가 다르게 정해질 수 있다는 문제가 있는거 같음
       - 특징 추출 : 기존에 존재하던 독립적인 특징들의 결합으로 '새로운 독립적인 특징'을 만드는 방법 
         - Linear : 주성분 분석(PCA), 특이값 분해(SVD) 등 기법들이 존재
         - Non-linear : Kernel PCA, t-SNE 등 있다고 함
     - 노이즈 처리 : 기법이 되게 다양함<Br>
       대표적인 내용으로 회귀 함수를 통해 하는 방법, 클러스터링을 하는 방법 등이 있음
4. 벡터 임베딩에서 데이터 비교
     - 유클리드 거리 : 서로 다른 벡터의 거리(d)를 통해 비교<br>
       크기에 민감하기 때문에 크기나 개수 등을 반영하는 데이터에 유용
     - 코사인 유사도(거리) : 두 벡터 사이의 각도에 대한 코사인을 정규화한 측정값<br>
       -1 ~ 1 사이의 값으로 1은 동일, 0은 직교(관련 없음), -1은 반대를 의미<br>
       유클리드 거리보다 학습 데이터의 빈도에 덜 민갑해서 NLP 작업에서 널리 사용된다고 함
     - dot : 각 벡터의 구성 요소의 곱의 합<br>
       빈도나 크기를 반영하는 코사인 거리의 비정규화 버전으로 보임
5. 벡터 임베딩 사용 어디서?
     - 이미지 : 컨볼루션, 패딩 등
     - NLP : 텍스트 저장할 때(단어, 문장, 문서 다 다른거 같음)
     - 오디오, 그래프 등

#### 주제 5: 자주보던 용어, 영어 단어가 뭐더라
- LLM : Large Language Model, 대규모 언어 모델
- NLP : Natural Language Processing, 자연어 처리
- RAG : Retrieval Augmented Generation, 검색 증강 생성
- PCA : Principal Component Analysis, 주성분 분석
- SVD : Singular Value Decomposition, 특이값 분해
- CNN : Convolution Neural Network, 합성곱 신경망 

### 오늘의 도전 과제와 해결 방법
- X

### 오늘의 회고
- 수학의 기본적인 개념들을 다시 복기하면서 정의나 이해와 관련하여 부족한 점이 많다 느꼈다
- 복습 중에 행렬과 벡터 사이에 이해가 매끄럽게 흘러가지 않아서 내용을 좀 더 찾아봤다
- 벡터 DB에 대해 찾아보면서 다양한 기법들이 많다는 것을 알 수 있었고, 막연히 알고 있던 내용들이 약간씩 연결되어 가는 기분이 들었다.

### 참고 자료 및 링크
- 백터 임베딩 관련
  - [IBM](https://www.ibm.com/kr-ko/think/topics/vector-embedding)
  - [RAG 관련 블로그](https://jerry-ai.com/13)

